# -*- coding: utf-8 -*-
"""bhagvad gita RAG bot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18I7514YsjfmUo64g1oBMRPaBg3vlCX9T
"""

# Commented out IPython magic to ensure Python compatibility.
!pip install chromadb
!pip install langchain langchain-community
!pip install langchain langchain_community chromadb tiktoken
!pip install python-dotenv
# %pip install -qU langchain_community beautifulsoup4 # To use the WebBaseLoader you first need to install the langchain-community python package.
!pip install transformers accelerate sentencepiece --upgrade
!pip install torch --index-url https://download.pytorch.org/whl/cpu  # change if you have GPU
!pip install -U langchain-chroma langchain-huggingface

"""# Load model Locally and RAG chatbot"""

from huggingface_hub import login
import getpass

# Secure prompt (invisible input)
hf_token = getpass.getpass("üîê Enter your Hugging Face token: ")

# Log in
login(hf_token)

"""## Document Loading & Chunks RAG retriever"""

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.document_loaders import TextLoader

# Load the Bhagavad Gita text
from langchain_community.document_loaders import WebBaseLoader

urls = [
    "https://sanskritdocuments.org/doc_giitaa/bhagvadnew.html",  # Bhagavad Gita
    "https://sanskritdocuments.org/doc_purana/bhagpur-00-mahatmyam.html", # SriBha-0
    "https://sanskritdocuments.org/doc_purana/bhagpur-01.html", # SriBha-1
    "https://sanskritdocuments.org/doc_purana/bhagpur-02.html", # SriBha-2
    "https://sanskritdocuments.org/doc_purana/bhagpur-03.html", # SriBha-3
    "https://sanskritdocuments.org/doc_purana/bhagpur-04.html", # SriBha-4
    "https://sanskritdocuments.org/doc_purana/bhagpur-05.html", # SriBha-5
    "https://sanskritdocuments.org/doc_purana/bhagpur-06.html", # SriBha-6
    "https://sanskritdocuments.org/doc_purana/bhagpur-07.html", # SriBha-7
    "https://sanskritdocuments.org/doc_purana/bhagpur-08.html", # SriBha-8
    "https://sanskritdocuments.org/doc_purana/bhagpur-09.html", # SriBha-9
    "https://sanskritdocuments.org/doc_purana/bhagpur-10a.html", # SriBha-10.1
    "https://sanskritdocuments.org/doc_purana/bhagpur-10b.html", # SriBha-10.2
    "https://sanskritdocuments.org/doc_purana/bhagpur-11.html", # SriBha-11
    "https://sanskritdocuments.org/doc_purana/bhagpur-12.html"  # SriBha-12
]

loaders = [WebBaseLoader(url) for url in urls]

documents = []
for loader in loaders:
    try:
        doc = loader.load()
        documents.extend(doc)
    except Exception as e:
        print(f"‚ùå Failed to load {loader}: {e}")

print(f"‚úÖ Total Documents Loaded: {len(documents)}")


# Split into chunks
splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
chunks = splitter.split_documents(documents)

print(f"‚úÖ Total Chunks: {len(chunks)}")

# Load embedding model
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# Save to ChromaDB
vectorstore = Chroma.from_documents(
    documents=chunks,
    embedding=embedding_model,
    persist_directory="./chromadb"
)

vectorstore.persist()
print(f"‚úÖ Vector store created with {len(chunks)} chunks.")

"""## Load Retriever and Connect Gemma-2B-IT LLM"""

from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings

embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
vectorstore = Chroma(persist_directory="./chromadb", embedding_function=embedding_model)
retriever = vectorstore.as_retriever()

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

model_id = "google/gemma-2-2b-it"

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
model.eval()

def generate_response(prompt, max_new_tokens=256):
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    with torch.no_grad():
        outputs = model.generate(**inputs, max_new_tokens=max_new_tokens)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

"""### Check response from model"""

print(generate_response("What is the essence of Bhagavad Gita?"))

"""## Integrate with Streamlit UI using Fastapi"""

# !pip install fastapi uvicorn pyngrok nest-asyncio transformers accelerate

